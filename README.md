# LocalPythonCodingLlm 

This extensision runs a LLM model locally with python.You can query it in the Chatwindow.

## Example usage
[The users asks: @LocalPythonCodingLLM /queryLLM Write a function to count the number of vowels in a given string. test {assert count_vowels("python") == 1, assert count_vowels("a") == 1. The model answers with correct code} ](exampleQuery.png)
## Features
- offline 
- no data collection
- open source 
- unlimited queries

keep in mind: Chatgpt/Copilot will be more accurate, since they have more processing power etc ...


## Requirements
- python in the PATH
- github.copilot-chat extension in VSC
## Issues

please report issues to https://github.com/manuel-Oelmaier/LocalPythonCodingLlm/issues

## Release Notes

### 0.0.1 extension release




